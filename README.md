# FindingFraudsters
Repo to analyze finding fraudsters dataset

Inspired by https://arxiv.org/pdf/2208.14417 and their git repo: https://github.com/amazon-science/fraud-dataset-benchmark which provides methods to prepare the datasets and reproduce results

Useful writeup of previous analysis of dataset: https://developer.nvidia.com/blog/leveraging-machine-learning-to-detect-fraud-tips-to-developing-a-winning-kaggle-solution/

## Brief Description 

• Developed a fraud detection model using boosted decision trees (BDT) on the IEEE-CIS Fraud Detection dataset.

• Dealt with a highly imbalanced dataset of 590,540 transactions, of which only 3.5% were fraudulent.

• Achieved 93% ROC AUC score using ensemble model of XGBoost, LightGBM and CATBoost

## Install Instructions
install conda python environment manager: https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html

To install the necessary packages, run: `conda env create -f environment.yml`

The contents of this package can be installed with pip. 
First `cd FindingFraudsters`
Then `pip install .` or `pip install . -e` if you want to edit the contents

## Run Instructions
After installation, the training script can be run using the `train-model` command. Use `train-model --help` to see run options.
An example command to train a model:
`train-model -t path/to/data/train_transaction.csv -i path/to/data/train_identity.csv  --train_test_split 0.8  --tr_cols path/to/data/Transaction_features.txt --id_cols path/to/data/id_features.txt` --model testModel.json

Running this will produce ROC curves, Precision-Recall ROC curves, a confusion matrix corresponding to the optimal ROC threshold, and the model output in the `output` directory

The test dataset does not include the `isFraud` column. Specifying the `--test` flag will ensure that the `isFraud` column is not read. It will also evaluate the input files and output a predictions csv which can be uploaded to kaggle. As no labels are assumed when predicting in this way, no output plots will be generated.

## Contents
Data Exploration notebooks are kept in the notebooks directory. They show the process used to filter and transform variables of interest

`data_pruning.py` contains functions used to filter out unwanted variables. Several methods were used and compared. Lists of useful variables were generated by several different functions. The lists were then compared to find which variables were selected more than once. If a variable was selected by a different method multiple times, it is used in the final training.

`train_model.py` is used to train the model. Data is loaded and augmented via the `data_loader` class. 

 Two notebooks `EDA.ipynb` and `FE_Training.ipynb` are included in the folder named `Notebooks`. Data exploratory analysis is done in `EDA.ipynb`, while feature engineering and training are done in `FE_Training.ipynb`.
